
# cs284a_hw3-1_webpage

## Project Overview

In this project, we have built a complete rendering using a path tracing algorithm step by step. From the beginning, we apply ray intersection with primitive. And then we use ***BVH*** (Bounding Volume Hierarchy) to speed up the process of calculating ray intersections. 

For the next part, we implement direct illumination and indirect illumination to calculate the lightings on the surface of the material. Basically, we divide lighting as zero bounce, one bounce and multiple bounces. Those combined enable realistic shading of the object, which makes rendered images more close to real-world objects than previous ones. 

For the last part, we implement adaptive sampling, which is a method to sample pixels to reduce noise. It avoids the problem of using high resolution per pixels and puts more importance on  points which converge slower. This gives us a way to get a tradeoff between high computations and low noise. 


## Part1: Ray Generation and Scene Intersection

### 1-1 Walk through the ray generation and primitive intersection parts of the rendering pipeline.

The ray generation process is as follows: 1. We map the points from normalized image space to sensor in camera space. 2. We then calculate the ray direction in the sensor space. 3. We then map the ray from the sensor space to the world space by the transformation matrix and position vector. 

Finally, for the primitive intersection part, we update information such as t-value of the ray, primitive points, and BSDF. For surface normals, we calculate the triangle’s normals by barycentric weights generated by the intersected point. For sphere intersection, the normals can be represented as the direction pointing from the sphere center to the intersection point. 

### 1-2 Explain the triangle intersection algorithm you implemented in your own words.

To check whether a triangle has an intersection point with a ray, we can first calculate the intersection point by the ***Moller Trumbore Algorithm***. And we can get the barycentric coordinates by the algorithm, assuming they are ***b1***, ***b2***, ***b3***. We check the coordinates to verify the intersection point is in the triangle. (all barycentric coordinates are within 0 and 1). Also we need to check the t-value of the intersection point is within the max_t and min_t range of the ray.

If the point is inside the triangle, we update ray’s max_t and store required information in the intersection object.

### 1-3 Show images with normal shading for a few small .dae files.

Below are some images renderd after finishing part1 (normal shading).

CBempty | CBspheres 
:---: | :---:
![](/pic/p1/CBempty.png) | ![](/pic/p1/CBspheres.png) 
teopot | banana 
![](/pic/p1/teopot.png) | ![](/pic/p1/banana.png) 


## Part2: Bounding Volume Hierarchy

### 2-1 Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.

For the construction process of BVH, it is a recursive process. We simply traverse the primitives and add them to the current leaf node if the number of primitives within the current bounding volume is less than the ***max_leaf_size*** predefined. Otherwise, we try to find an optimal split method and recursively call the function to get the left and right child of the current BVH node. 

As for the heuristic of choosing the splitting point, we calculate the centroid coordinates of the current BVH node by averaging the centroids of its primitives’ bbox. And we divide all of the current node’s primitives into left and right sides. We calculate the difference between the number of primitives which belong to left and right. The optimal partition is the one which splits most evenly, meaning the difference between left and right is smalleest. 

### 2-2 Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.

maxplanck | beast | CBlucy
:---: | :---: | :---:
![](/pic/p2/maxplanck.png) | ![](/pic/p2/beast.png) | ![](/pic/p2/CBlucy.png)


For the three images above:
 - Maxplanck: over 50000 primitives, hard to render without BVH accelerating. But it only took 0.1680s to render with BVH acceleration. 
 - Beast: over 60000 primitives,  hard to render without BVH accelerating. But it only took 0.1405sto render with BVH acceleration. 
 - CBlucy: over 130000 primitives,  hard to render without BVH accelerating. But it only took 0.2655s to render with BVH acceleration. 

### 2-3 Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.


\ | Raw | With BVH 
:---: | :---: | :---:
beast | ![](/pic/p2/time_beast1.png) | ![](/pic/p2/time_beast2.png) 
teapot | ![](/pic/p2/time_teapot1.png) | ![](/pic/p2/time_teapot2.png) 

We compare the performance on rendering some moderately complex geometries. From the image above, we can see that it took 54s to render the beast.dae file with over 60000 ptimitives. But it only took 0.1405s with BVH acceleration, which is a nearly 400x speedup. And it took 22s to render the teopot.dae file with over 2000 primitves. But it only took 0.06s with BVH acceleration, which is a 1500x speedup. 

Overall, the speedup of rendering bought by BVH acceleration range from 100x to 1000x dependent on the structure and distribution of the primitves a .dae file presents. 

## Part3: Direct Illumination

### 3-1 Walk through both implementations of the direct lighting function.
There are two implementations for direct lighting are included in this assignment. The difference between these two lies in the sampling method for the incident angle.\

Uniform Hemisphere sampling: In this kind of sampling, because the detected light and the light source are not necessarily in the same direction, many detected light do not play a role in the reflected light equation. In the actual implementation, we need to simulate the inverse process of the whole process, that is, tracking it from the opposite direction of light rays. Once the tracked rays have intersection in the scene, we will calculate the corresponding ray influence, update the color around the pixel in the scene, and then repeat this process repeatedly until each ray has been tracked. The correct handling of intersections is very important in code implementation. We not only need to calculate how much light converges at intersections, but also need to integrate over all the light arriving in a hemisphere around the point of interest. Monte Carlo estimator will be used in this integration process. After we traverse all incoming lights, we can use the reflection equation to calculate the illumination of the whole scene, and finally update the EST_radiance_global_Illumination to get the results.\

Importance sampling: Different from uniform hemisphere sampling, importance sampling can ensure that each tracked light is directed to the light source. The only thing to consider is object occlusion. Therefore, under the same number of samples, importance sampling will be brighter than uniform hemisphere sampling. Because there will be no loss of light, there will be less noise than uniform hemisphere sampling. In the actual implementation process, we will directly sample all lights instead of only sampling in the hemisphere directions, which will be easier to implement than the previous sampling method, because each light in the scene can successfully reach the irradiation point. We need to bring all lights into the reflection equation to calculate the total light output. Finally, we also need to update the EST_radiance_global_Illumination to get the results. And the value of direct_hemisphere_sample is used to determine which kind of sampling should be used.

### 3-2 Show some images rendered with both implementations of the direct lighting function.
Bunny:\
![3-2-1](/pic/p3/3-2-1.png)\
Dragon:\
![3-2-2](/pic/p3/3-2-2.png)\
Shperes:\
![3-2-6](/pic/p3/3-2-6.png)
### 3-3 Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
1 light ray:\
![3-3-1](/pic/p3/3-3-1.png)\
4 light rays:\
![3-3-2](/pic/p3/3-3-2.png)\
16 light rays:\
![3-3-3](/pic/p3/3-3-3.png)\
64 light rays:\
![3-3-4](/pic/p3/3-3-4.png)\
### 3-4 Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

## Part4: Global Illumination

### 4-1 Walk through your implementation of the indirect lighting function.

### 4-2 Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.

### 4-3 Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel.

### 4-4 For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.

### 4-5 Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.

### 4-6 You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours.

## Part5: Adaptive Sampling

### 5-1 Walk through your implementation of the adaptive sampling.

### 5-2 Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.

## Extra Credits





For more details see [Basic writing and formatting syntax](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).


