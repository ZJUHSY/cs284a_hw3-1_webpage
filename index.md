
# cs284a_hw3-1_webpage

## Project Overview

In this project, we have built a complete rendering using a path tracing algorithm step by step. From the beginning, we apply ray intersection with primitive. And then we use ***BVH*** (Bounding Volume Hierarchy) to speed up the process of calculating ray intersections. 

For the next part, we implement direct illumination and indirect illumination to calculate the lightings on the surface of the material. Basically, we divide lighting as zero bounce, one bounce and multiple bounces. Those combined enable realistic shading of the object, which makes rendered images more close to real-world objects than previous ones. 

For the last part, we implement adaptive sampling, which is a method to sample pixels to reduce noise. It avoids the problem of using high resolution per pixels and puts more importance on  points which converge slower. This gives us a way to get a tradeoff between high computations and low noise. 


## Part1: Ray Generation and Scene Intersection

### 1-1 Walk through the ray generation and primitive intersection parts of the rendering pipeline.

The ray generation process is as follows: 1. We map the points from normalized image space to sensor in camera space. 2. We then calculate the ray direction in the sensor space. 3. We then map the ray from the sensor space to the world space by the transformation matrix and position vector. 

Finally, for the primitive intersection part, we update information such as t-value of the ray, primitive points, and BSDF. For surface normals, we calculate the triangle’s normals by barycentric weights generated by the intersected point. For sphere intersection, the normals can be represented as the direction pointing from the sphere center to the intersection point. 

### 1-2 Explain the triangle intersection algorithm you implemented in your own words.

To check whether a triangle has an intersection point with a ray, we can first calculate the intersection point by the ***Moller Trumbore Algorithm***. And we can get the barycentric coordinates by the algorithm, assuming they are ***b1***, ***b2***, ***b3***. We check the coordinates to verify the intersection point is in the triangle. (all barycentric coordinates are within 0 and 1). Also we need to check the t-value of the intersection point is within the max_t and min_t range of the ray.

If the point is inside the triangle, we update ray’s max_t and store required information in the intersection object.

### 1-3 Show images with normal shading for a few small .dae files.

Below are some images renderd after finishing part1 (normal shading).

CBempty | CBspheres 
:---: | :---:
![](/pic/p1/CBempty.png) | ![](/pic/p1/CBspheres.png) 
teopot | banana 
![](/pic/p1/teopot.png) | ![](/pic/p1/banana.png) 


## Part2: Bounding Volume Hierarchy

### 2-1 Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.

For the construction process of BVH, it is a recursive process. We simply traverse the primitives and add them to the current leaf node if the number of primitives within the current bounding volume is less than the ***max_leaf_size*** predefined. Otherwise, we try to find an optimal split method and recursively call the function to get the left and right child of the current BVH node. 

As for the heuristic of choosing the splitting point, we calculate the centroid coordinates of the current BVH node by averaging the centroids of its primitives’ bbox. And we divide all of the current node’s primitives into left and right sides. We calculate the difference between the number of primitives which belong to left and right. The optimal partition is the one which splits most evenly, meaning the difference between left and right is smalleest. 

Notice, we have to also deal with the situation where one side has 0 primitive. This will definitely cause inifite loop because when we enter the next level of function call we get the same node with same number of leafs (no distribution between left and right nodes). Therefore, we introduce extra logics: We sort the all primitives within the current node by its x-coordinates of the centroid, and split in the middle iterator to prevent segmentation fault. 

### 2-2 Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.

maxplanck | beast | CBlucy
:---: | :---: | :---:
![](/pic/p2/maxplanck.png) | ![](/pic/p2/beast.png) | ![](/pic/p2/CBlucy.png)


For the three images above:
 - Maxplanck: over 50000 primitives, hard to render without BVH accelerating. But it only took 0.1680s to render with BVH acceleration. 
 - Beast: over 60000 primitives,  hard to render without BVH accelerating. But it only took 0.1405sto render with BVH acceleration. 
 - CBlucy: over 130000 primitives,  hard to render without BVH accelerating. But it only took 0.2655s to render with BVH acceleration. 

### 2-3 Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.


\ | Raw | With BVH 
:---: | :---: | :---:
beast | ![](/pic/p2/time_beast1.png) | ![](/pic/p2/time_beast2.png) 
teapot | ![](/pic/p2/time_teapot1.png) | ![](/pic/p2/time_teapot2.png) 

We compare the performance on rendering some moderately complex geometries. From the image above, we can see that it took 54s to render the beast.dae file with over 60000 ptimitives. But it only took 0.1405s with BVH acceleration, which is a nearly 400x speedup. And it took 22s to render the teopot.dae file with over 2000 primitves. But it only took 0.06s with BVH acceleration, which is a 1500x speedup. 

Overall, the speedup of rendering bought by BVH acceleration range from 100x to 1000x dependent on the structure and distribution of the primitves a .dae file presents. 

## Part3: Direct Illumination

### 3-1 Walk through both implementations of the direct lighting function.
There are two implementations for direct lighting are included in this assignment. The difference between these two lies in the sampling method for the incident angle.\

Uniform Hemisphere sampling: In this kind of sampling, because the detected light and the light source are not necessarily in the same direction, many detected light do not play a role in the reflected light equation. In the actual implementation, we need to simulate the inverse process of the whole process, that is, tracking it from the opposite direction of light rays. Once the tracked rays have intersection in the scene, we will calculate the corresponding ray influence, update the color around the pixel in the scene, and then repeat this process repeatedly until each ray has been tracked. The correct handling of intersections is very important in code implementation. We not only need to calculate how much light converges at intersections, but also need to integrate over all the light arriving in a hemisphere around the point of interest. Monte Carlo estimator will be used in this integration process. After we traverse all incoming lights, we can use the reflection equation to calculate the illumination of the whole scene, and finally update the EST_radiance_global_Illumination to get the results.\

Importance sampling: Different from uniform hemisphere sampling, importance sampling can ensure that each tracked light is directed to the light source. The only thing to consider is object occlusion. Therefore, under the same number of samples, importance sampling will be brighter than uniform hemisphere sampling. Because there will be no loss of light, there will be less noise than uniform hemisphere sampling. In the actual implementation process, we will directly sample all lights instead of only sampling in the hemisphere directions, which will be easier to implement than the previous sampling method, because each light in the scene can successfully reach the irradiation point. We need to bring all lights into the reflection equation to calculate the total light output. Finally, we also need to update the EST_radiance_global_Illumination to get the results. And the value of direct_hemisphere_sample is used to determine which kind of sampling should be used.

### 3-2 Show some images rendered with both implementations of the direct lighting function.

Bunny | Dragon | Shperes
:---: | :---: | :---:
![3-2-1](/pic/p3/3-2-1.png) | ![3-2-2](/pic/p3/3-2-2.png) | ![3-2-6](/pic/p3/3-2-6.png)

### 3-3 Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.

 Light Ray | 1 | 4 | 16 | 64
 :---: | :---: | :---: | :---: | :---:
 bunny | ![3-3-1](/pic/p3/3-3-1.png) | ![3-3-2](/pic/p3/3-3-2.png) | ![3-3-3](/pic/p3/3-3-3.png) | ![3-3-4](/pic/p3/3-3-4.png)

### 3-4 Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

Results of uniform hemisphere sampling:\

 \ | 16 light rays | 64 light rays 
:---: | :---: | :---:
Uniform Hemisphere Sampling | ![3-3-6](/pic/p3/3-3-6.png) | ![3-3-5](/pic/p3/3-3-5.png)
Lighting Sampling | ![3-3-3](/pic/p3/3-3-3.png) | ![3-3-4](/pic/p3/3-3-4.png)

Analysis:\
As can be seen from the figure, compared with lighting sampling, uniform hemisphere sampling has more noise, and the overall light is more uniform. Both will become clearer as the number of samples increases. The clarity of uniform hemisphere sampling changes more obviously with the number of samples.

## Part4: Global Illumination

### 4-1 Walk through your implementation of the indirect lighting function.
For direct illumination, we only calculate the light that bounces zero or once. To add extra indirect illumination, we set m (the maximum depth of the ray), and sample incoming ray direction from the diffused materias. 

In the main body of this part, ***at_least_one_bounce_radiance***, we call the one_bounce_radiance recursively until the reflected ray’s depth is lower than 2 or there are no intersections between the objects and the ray. We add up the indirect illumination using monte-carlo simulation. Notice, the function should do nothing if m is set to 0, because there should be no bounced lightings. 

Finally, In the wrapper function called ***est_radiance_global_illumination***, we add up the direct illumination and indirect illumination to adjust the material lightings. 

### 4-2 Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
 
 Below are some example rendered by global illumination. 
 
sphere | CBunny | bunny | dragon
:---: | :---: | :---: | :---:
![](/pic/p4/4-2-1.png) | ![](/pic/p4/4-2-2.png) | ![](/pic/p4/4-2-3.png) | ![](/pic/p4/4-2-4.png)

### 4-3 Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel.

We pick ../dae/sky/CBspheres_lambertian.dae for testing. For the direct illumination, we only include zero or one bounce. For the indirect intersections, we only include one or more bounce. For global illumniation, we combine the two. 

We can see from the results as below. The indirect illumination has a dark center but light surroudnings. The direct illumination has a light center and dark surroundings. The global illumination has an overall lighed ceilings. And also the surface of the spheres are darker for direct illumniation compared to global and indirect illuminiation.

\ | global | indirect | direct
:---: | :---: | :---: | :---: 
spheres | ![](/pic/p4/CBspheres_global.png) | ![](/pic/p4/CBspheres_indirect.png) | ![](/pic/p4/CBspheres_direct.png)

### 4-4 For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.

We can see from below that we only get zero-boucne illumniation when m=0, the whole scene is dark excpet the ceilings. And when we increase the maximum depth of the ray, the whole scene gets lighter with more indirect illumination. 

\ | m=0 | m=1 | m=2 | m=100
:---: | :---: | :---: | :---: | :---:
bunny | ![](/pic/p4/CBunny_m0.png) | ![](/pic/p4/CBunny_m1.png) | ![](/pic/p4/CBunny_m2.png) | ![](/pic/p4/CBunny_m100.png)

### 4-5 Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.

We pick bench.dae and compare the magic of various sample-per-pixel rates. We can see that the rendered views' noise decrease as the sample-per-pixel increases. 

\ | s=1 | s=2 | s=4 | s=8 
:---: | :---: | :---: | :---: | :---:
bench | ![](/pic/p4/4-4-1.png) | ![](/pic/p4/4-4-2.png) | ![](/pic/p4/4-4-3.png) | ![](/pic/p4/4-4-4.png)

\ | s=16 | s=64 | s=1024 
:---: | :---: | :---: | :---:
bench | ![](/pic/p4/4-4-5.png) | ![](/pic/p4/4-4-6.png) | ![](/pic/p4/4-4-7.png) 


## Part5: Adaptive Sampling

### 5-1 Walk through your implementation of the adaptive sampling.
In light sampling, the noise in the scene is always difficult to avoid. Although the noise can be eliminated by increasing the number of samples, increasing the number of samples globally will add a lot of unnecessary computational costs. We often do not need to uniformly increase the sampling amount of all pixels, but dynamically allocate according to the specific situation of the image, which is adaptive sampling. The adaptive sampling we implement is to check whether I is within the maximum tolerance through the sample mean and variance to measure the pixel's conversion I to determine whether it is less than the tolerance.

### 5-2 Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.

Noise-free Rendered Result:\
![bunny.png](/pic/p5/bunny.png)\
Sample Rate Image:\
![bunny_rate.png](/pic/p5/bunny_rate.png)\
## Extra Credits
### Soup up our naive parallelization or use some clever arithmetic tricks in your inner loops (probably ray intersection) to speed things up. Profile your code and find the bottlenecks. Tell us about anything that gives a significant increase in rays/sec from your initial implementation.
In part1 Task3, when we implemented Ray-Triangle Intersection, we used Moller Trumbore Algorithm to speed up the intersection detection.\
The conventional calculation method is as follows:
![extra3](/pic/extra3.png)\
As taught in lecture, Moller Trumbore Algorithm is a method to quickly calculate the intersection of rays and triangles in three dimensions. Through vector and matrix calculation, the coordinates of intersection and center of gravity can be quickly obtained without pre calculation of plane equation containing triangles.\
![extra1](/pic/extra1.png)\
Our code implementation is shown below:\
![extra2](/pic/extra4.png)\
Moller Trumbore Algorithm reduces the computational complexity of conventional intersection judgment to 7 times of multiplication, and improves the running efficiency of our code by 50%.
Our website is hosted at [webiste](https://zjuhsy.github.io/cs284a_hw3-1_webpage/)

For more details see [Basic writing and formatting syntax](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).


