
# cs284a_hw3-1_webpage

## Project Overview


## Part1: Ray Generation and Scene Intersection

### 1-1 Walk through the ray generation and primitive intersection parts of the rendering pipeline.

The ray generation process is as follows: 1. We map the points from normalized image space to sensor in camera space. 2. We then calculate the ray direction in the sensor space. 3. We then map the ray from the sensor space to the world space by the transformation matrix and position vector. 

Finally, for the primitive intersection part, we update information such as t-value of the ray, primitive points, and BSDF. For surface normals, we calculate the triangle’s normals by barycentric weights generated by the intersected point. For sphere intersection, the normals can be represented as the direction pointing from the sphere center to the intersection point. 

### 1-2 Explain the triangle intersection algorithm you implemented in your own words.

To check whether a triangle has an intersection point with a ray, we can first calculate the intersection point by the ***Moller Trumbore Algorithm***. And we can get the barycentric coordinates by the algorithm, assuming they are ***b1***, ***b2***, ***b3***. We check the coordinates to verify the intersection point is in the triangle. (all barycentric coordinates are within 0 and 1). Also we need to check the t-value of the intersection point is within the max_t and min_t range of the ray.

If the point is inside the triangle, we update ray’s max_t and store required information in the intersection object.

### 1-3 Show images with normal shading for a few small .dae files.

Below are some images renderd after finishing part1 (normal shading).

<img src="/pic/p1/CBempty.png" width="45%"/> <img src="/pic/p1/CBspheres.png" width="45%"/> 
<img src="/pic/p1/teopot.png" width="45%"/> <img src="/pic/p1/banana.png" width="45%"/> 

## Part2: Bounding Volume Hierarchy

### 2-1 Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.

For the construction process of BVH, it is a recursive process. We simply traverse the primitives and add them to the current leaf node if the number of primitives within the current bounding volume is less than the ***max_leaf_size*** predefined. Otherwise, we try to find an optimal split method and recursively call the function to get the left and right child of the current BVH node. 

As for the heuristic of choosing the splitting point, we calculate the centroid coordinates of the current BVH node by averaging the centroids of its primitives’ bbox. And we divide all of the current node’s primitives into left and right sides. We calculate the difference between the number of primitives which belong to left and right. The optimal partition is the one which splits most evenly, meaning the difference between left and right is smalleest. 

### 2-2 Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.

<img src="/pic/p2/maxplanck.png" width="30%"/> <img src="/pic/p2/beast.png" width="30%"/> <img src="/pic/p2/CBlucy.png" width="30%"/> 

For the three images above:
 - Maxplanck: over 50000 primitives, hard to render without BVH accelerating. But it only took 0.1680s to render with BVH acceleration. 
 - Beast: over 60000 primitives,  hard to render without BVH accelerating. But it only took 0.1405sto render with BVH acceleration. 
 - CBlucy: over 130000 primitives,  hard to render without BVH accelerating. But it only took 0.2655s to render with BVH acceleration. 

### 2-3 Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.

<img src="/pic/p2/time_beast1.png" width="45%"/> <img src="/pic/p2/time_beast2.png" width="45%"/> 
<img src="/pic/p2/time_teapot1.png" width="45%"/> <img src="/pic/p2/time_teapot2.png" width="45%"/> 

We compare the performance on rendering some moderately complex geometries. From the image above, we can see that it took 54s to render the beast.dae file with over 60000 ptimitives. But it only took 0.1405s with BVH acceleration, which is a nearly 400x speedup. And it took 22s to render the teopot.dae file with over 2000 primitves. But it only took 0.06s with BVH acceleration, which is a 1500x speedup. 

Overall, the speedup of rendering bought by BVH acceleration range from 100x to 1000x dependent on the structure and distribution of the primitves a .dae file presents. 

## Part3: Direct Illumination

### 3-1 Walk through both implementations of the direct lighting function.

### 3-2 Show some images rendered with both implementations of the direct lighting function.

### 3-3 Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.

### 3-4 Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

## Part4: Global Illumination

### 4-1 Walk through your implementation of the indirect lighting function.

### 4-2 Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.

### 4-3 Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel.

### 4-4 For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.

### 4-5 Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.

### 4-6 You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours.

## Part5: Adaptive Sampling

### 5-1 Walk through your implementation of the adaptive sampling.

### 5-2 Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.

## Extra Credits





For more details see [Basic writing and formatting syntax](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).


